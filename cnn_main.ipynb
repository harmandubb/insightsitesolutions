{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from multi_digit_cnn import MultiDigitCNN  # Import the CNN model\n",
    "from image_preprocessing import preprocess_image  # Import preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SVHN dataset with preprocessing pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.Resize((1024, 1024)),  # Resize as per the paper\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1,1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "columns = [\"filename\", \"labels\"]\n",
    "df_train = pd.DataFrame(columns=columns)\n",
    "df_test = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Define path to digitStruct.mat\n",
    "train_file_path = \"data/train/digitStruct.mat\"  # Update with actual path\n",
    "test_file_path = \"data/test/digitStruct.mat\"\n",
    "\n",
    "f = h5py.File(train_file_path, 'r')\n",
    "bbox_train_dataset = f.get('digitStruct/bbox')\n",
    "num_train_images = len(bbox_train_dataset)\n",
    "\n",
    "\n",
    "def extract_labels(bbox_dataset, img_num):\n",
    "    bbox_ref = bbox_dataset[img_num][0]\n",
    "    label_ref = f[bbox_ref][\"label\"]\n",
    "\n",
    "    # Handle single-label case (directly stored as a number)\n",
    "    if label_ref.shape[0] == 1:  # Shape is empty -> single value\n",
    "        labels = np.array([int(label_ref[()].item())])  # Convert directly\n",
    "    else:\n",
    "        # Multiple labels (stored as references)\n",
    "        labels = np.array([int(f[label_ref[i][0]][()].item()) for i in range(label_ref.shape[0])])\n",
    "        \n",
    "    return labels\n",
    "\n",
    "# Loop through images and print their labels\n",
    "for i in range(num_train_images):\n",
    "    df_train.loc[len(df_train)] = [f\"{i+1}.png\", extract_labels(bbox_train_dataset, i)]   \n",
    "\n",
    "df_train.to_csv(\"data/train/cleaned_train_labels.csv\")\n",
    "\n",
    "f = h5py.File(test_file_path, 'r')\n",
    "bbox_test_dataset = f.get('digitStruct/bbox')\n",
    "num_test_images = len(bbox_test_dataset)\n",
    "\n",
    "for i in range(num_test_images):\n",
    "    df_test.loc[len(df_test)] = [f\"{i+1}.png\", extract_labels(bbox_test_dataset, i)]  \n",
    "\n",
    "\n",
    "df_train.to_csv(\"data/test/cleaned_test_labels.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class SVHNDataset(Dataset):\n",
    "    def __init__(self, root_dir, annotations, transform=None, max_digits=12, pad_value=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to the directory containing images.\n",
    "            annotations (DataFrame): DataFrame with 'filename' and 'labels'.\n",
    "            transform (callable, optional): Image transformations.\n",
    "            max_digits (int): Maximum number of digits per image.\n",
    "            pad_value (int): Value used for padding shorter label sequences.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.annotations = annotations\n",
    "        self.transform = transform\n",
    "        self.max_digits = max_digits\n",
    "        self.pad_value = pad_value\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_name = os.path.join(self.root_dir, self.annotations.iloc[idx][\"filename\"])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "\n",
    "        # Load multi-digit label\n",
    "        labels = self.annotations.iloc[idx][\"labels\"]\n",
    "\n",
    "        # Convert labels to fixed-length tensor (with padding)\n",
    "        label_tensor = torch.full((self.max_digits,), self.pad_value, dtype=torch.long)  # Initialize padding\n",
    "        label_tensor[:len(labels)] = torch.tensor(labels, dtype=torch.long)  # Fill with actual labels\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([64, 1, 1024, 1024])\n",
      "Labels batch shape: torch.Size([64, 12])\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import re\n",
    "# Define dataset directory and DataFrame (df_corrected contains filename-label mapping)\n",
    "image_train_dir = \"data/train\"  # Change this to your actual image directory\n",
    "train_annotations_path = \"data/train/cleaned_train_labels.csv\"\n",
    "\n",
    "image_test_dir = \"data/test\"  # Change this to your actual image directory\n",
    "test_annotations_path = \"data/test/cleaned_test_labels.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_annotations_path)\n",
    "df_test = pd.read_csv(test_annotations_path)\n",
    "\n",
    "def fix_label_format(label):\n",
    "    if isinstance(label, str):\n",
    "        # Replace spaces with commas inside brackets (fix missing commas)\n",
    "        fixed_label = re.sub(r\"\\[([0-9\\s]+)\\]\", lambda m: \"[\" + \",\".join(m.group(1).split()) + \"]\", label)\n",
    "        return fixed_label\n",
    "    return label\n",
    "\n",
    "df_train[\"labels\"] = df_train[\"labels\"].apply(fix_label_format)\n",
    "df_train[\"labels\"] = df_train[\"labels\"].apply(ast.literal_eval)\n",
    "\n",
    "df_test[\"labels\"] = df_test[\"labels\"].apply(fix_label_format)\n",
    "df_test[\"labels\"] = df_test[\"labels\"].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = SVHNDataset(root_dir=image_train_dir, annotations=df_train, transform=transform)\n",
    "test_dataset = SVHNDataset(root_dir=image_test_dir, annotations=df_test, transform=transform)\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "# Get a batch to verify\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Display shapes\n",
    "print(\"Image batch shape:\", images.shape)  # Expected: (batch_size, 3, 64, 64)\n",
    "print(\"Labels batch shape:\", labels.shape)  # Expected: (batch_size, max_digits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
