{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from multi_digit_cnn import MultiDigitCNN  # Import the CNN model\n",
    "from image_preprocessing import preprocess_image  # Import preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "columns = [\"filename\", \"labels\"]\n",
    "df_train = pd.DataFrame(columns=columns)\n",
    "df_test = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Define path to digitStruct.mat\n",
    "train_file_path = \"data/train/digitStruct.mat\"  # Update with actual path\n",
    "test_file_path = \"data/test/digitStruct.mat\"\n",
    "\n",
    "f = h5py.File(train_file_path, 'r')\n",
    "bbox_train_dataset = f.get('digitStruct/bbox')\n",
    "num_train_images = len(bbox_train_dataset)\n",
    "\n",
    "\n",
    "def extract_labels(bbox_dataset, img_num):\n",
    "    bbox_ref = bbox_dataset[img_num][0]\n",
    "    label_ref = f[bbox_ref][\"label\"]\n",
    "\n",
    "    # Handle single-label case (directly stored as a number)\n",
    "    if label_ref.shape[0] == 1:  # Shape is empty -> single value\n",
    "        labels = np.array([int(label_ref[()].item())])  # Convert directly\n",
    "    else:\n",
    "        # Multiple labels (stored as references)\n",
    "        labels = np.array([int(f[label_ref[i][0]][()].item()) for i in range(label_ref.shape[0])])\n",
    "        \n",
    "    return labels\n",
    "\n",
    "# Loop through images and print their labels\n",
    "for i in range(num_train_images):\n",
    "    df_train.loc[len(df_train)] = [f\"{i+1}.png\", extract_labels(bbox_train_dataset, i)]   \n",
    "\n",
    "df_train.to_csv(\"data/train/cleaned_train_labels.csv\")\n",
    "\n",
    "f = h5py.File(test_file_path, 'r')\n",
    "bbox_test_dataset = f.get('digitStruct/bbox')\n",
    "num_test_images = len(bbox_test_dataset)\n",
    "\n",
    "for i in range(num_test_images):\n",
    "    df_test.loc[len(df_test)] = [f\"{i+1}.png\", extract_labels(bbox_test_dataset, i)]  \n",
    "\n",
    "\n",
    "df_train.to_csv(\"data/test/cleaned_test_labels.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SVHN dataset with preprocessing pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.Resize((1024, 1024)),  # Resize as per the paper\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1,1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class SVHNDataset(Dataset):\n",
    "    def __init__(self, root_dir, annotations, transform=None, max_digits=18, pad_value=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to the directory containing images.\n",
    "            annotations (DataFrame): DataFrame with 'filename' and 'labels'.\n",
    "            transform (callable, optional): Image transformations.\n",
    "            max_digits (int): Maximum number of digits per image.\n",
    "            pad_value (int): Value used for padding shorter label sequences.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.annotations = annotations\n",
    "        self.transform = transform\n",
    "        self.max_digits = max_digits\n",
    "        self.pad_value = pad_value\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_name = os.path.join(self.root_dir, self.annotations.iloc[idx][\"filename\"])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "\n",
    "        # Load multi-digit label\n",
    "        labels = self.annotations.iloc[idx][\"labels\"]\n",
    "\n",
    "        # Convert labels to fixed-length tensor (with padding)\n",
    "        label_tensor = torch.full((self.max_digits,), self.pad_value, dtype=torch.long)  # Initialize padding\n",
    "        label_tensor[:len(labels)] = torch.tensor(labels, dtype=torch.long)  # Fill with actual labels\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Get a batch to verify\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Display shapes\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage batch shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, images\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Expected: (batch_size, 3, 64, 64)\u001b[39;00m\n",
      "File \u001b[0;32m~/Git_Projects/insightsitesolutions/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:491\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git_Projects/insightsitesolutions/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:422\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git_Projects/insightsitesolutions/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1107\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_init_fn \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m   1100\u001b[0m         _sharding_worker_init_fn,\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_init_fn,\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_world_size,\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rank,\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# No certainty which module multiprocessing_context is\u001b[39;00m\n\u001b[0;32m-> 1107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_result_queue \u001b[38;5;241m=\u001b[39m \u001b[43mmultiprocessing_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQueue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[var-annotated]\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_pids_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/multiprocessing/context.py:103\u001b[0m, in \u001b[0;36mBaseContext.Queue\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Returns a queue object'''\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqueues\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Queue\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQueue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/multiprocessing/queues.py:43\u001b[0m, in \u001b[0;36mQueue.__init__\u001b[0;34m(self, maxsize, ctx)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maxsize \u001b[38;5;241m=\u001b[39m maxsize\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writer \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mPipe(duplex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock \u001b[38;5;241m=\u001b[39m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opid \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/multiprocessing/context.py:68\u001b[0m, in \u001b[0;36mBaseContext.Lock\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Returns a non-recursive lock object'''\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msynchronize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Lock\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/multiprocessing/synchronize.py:169\u001b[0m, in \u001b[0;36mLock.__init__\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, ctx):\n\u001b[0;32m--> 169\u001b[0m     \u001b[43mSemLock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSEMAPHORE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/multiprocessing/synchronize.py:57\u001b[0m, in \u001b[0;36mSemLock.__init__\u001b[0;34m(self, kind, value, maxvalue, ctx)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m         sl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_semlock \u001b[38;5;241m=\u001b[39m \u001b[43m_multiprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSemLock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[43munlink_now\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import re\n",
    "import pandas as pd\n",
    "# Define dataset directory and DataFrame (df_corrected contains filename-label mapping)\n",
    "image_train_dir = \"data/train\"  # Change this to your actual image directory\n",
    "train_annotations_path = \"data/train/cleaned_train_labels.csv\"\n",
    "\n",
    "image_test_dir = \"data/test\"  # Change this to your actual image directory\n",
    "test_annotations_path = \"data/test/cleaned_test_labels.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_annotations_path)\n",
    "df_test = pd.read_csv(test_annotations_path)\n",
    "\n",
    "def fix_label_format(label):\n",
    "    if isinstance(label, str):\n",
    "        # Replace spaces with commas inside brackets (fix missing commas)\n",
    "        fixed_label = re.sub(r\"\\[([0-9\\s]+)\\]\", lambda m: \"[\" + \",\".join(m.group(1).split()) + \"]\", label)\n",
    "        return fixed_label\n",
    "    return label\n",
    "\n",
    "df_train[\"labels\"] = df_train[\"labels\"].apply(fix_label_format)\n",
    "df_train[\"labels\"] = df_train[\"labels\"].apply(ast.literal_eval)\n",
    "\n",
    "df_test[\"labels\"] = df_test[\"labels\"].apply(fix_label_format)\n",
    "df_test[\"labels\"] = df_test[\"labels\"].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = SVHNDataset(root_dir=image_train_dir, annotations=df_train, transform=transform)\n",
    "test_dataset = SVHNDataset(root_dir=image_test_dir, annotations=df_test, transform=transform)\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "# Get a batch to verify\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Display shapes\n",
    "print(\"Image batch shape:\", images.shape)  # Expected: (batch_size, 3, 64, 64)\n",
    "print(\"Labels batch shape:\", labels.shape)  # Expected: (batch_size, max_digits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 18, 10])\n"
     ]
    }
   ],
   "source": [
    "# Testin gmodel if it works with dummy tensor\n",
    "import torch\n",
    "from multi_digit_cnn import MultiDigitCNN  # Import the CNN model\n",
    "\n",
    "dummy_input = torch.randn(1, 1, 1024, 1024)\n",
    "model = MultiDigitCNN()\n",
    "output = model(dummy_input)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 24107) is killed by signal: Killed. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmulti_digit_cnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiDigitCNN  \u001b[38;5;66;03m# Import the CNN model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtraining_multi_digit_cnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_model\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMultiDigitCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#setting up training\u001b[39;00m\n\u001b[1;32m      9\u001b[0m train_model(model, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,test_loader\u001b[38;5;241m=\u001b[39mtest_loader, save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdigits_model_Feb_16, 2025\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Git_Projects/insightsitesolutions/multi_digit_cnn.py:54\u001b[0m, in \u001b[0;36mMultiDigitCNN.__init__\u001b[0;34m(self, max_digits, num_classes, dropout_rate)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool8 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMaxPool2d(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout8 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(p\u001b[38;5;241m=\u001b[39mdropout_rate)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_flatten_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_size, \u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m512\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_digits\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes)\n",
      "File \u001b[0;32m~/Git_Projects/insightsitesolutions/multi_digit_cnn.py:84\u001b[0m, in \u001b[0;36mMultiDigitCNN._get_flatten_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     83\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m)  \u001b[38;5;66;03m# Simulated input image\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     85\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool2(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))))\n\u001b[1;32m     86\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool3(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x))))\n",
      "File \u001b[0;32m~/Git_Projects/insightsitesolutions/venv/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py:73\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(previous_handler)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 24107) is killed by signal: Killed. "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from multi_digit_cnn import MultiDigitCNN  # Import the CNN model\n",
    "from training_multi_digit_cnn import train_model\n",
    "\n",
    "model = MultiDigitCNN()\n",
    "\n",
    "#setting up training\n",
    "\n",
    "train_model(model, batch_size=32, learning_rate=0.0001, train_loader=train_loader,test_loader=test_loader, save_path=\"digits_model_Feb_16, 2025\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
